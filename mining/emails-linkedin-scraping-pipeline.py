import os.path
import pandas as pd
from BrowserFactory import BrowserFactory
from BookmarkRepo import BookmarkRepo
from LinkedInProfileScraper import LinkedInProfileScraper
from LinkedInCompanyScraper import LinkedInCompanyScraper, EmptyScrapedCompany
import time
import json
import traceback
from tqdm import tqdm
import sys
from Generalizer import Generalizer

root_folder = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
DATA_FOLDER = os.path.join(root_folder, "data")

def load_config():
    with open(os.path.join(os.path.dirname(__file__), 'network_mining_config.json')) as f:
        return json.load(f)

def load_profile_links(config):
    source_file = os.path.join(DATA_FOLDER, config["crawler_result_file_name"])
    if not os.path.isfile(source_file):
        raise Exception("crawler_result_file_name is not found by path:\n {}".format(source_file))
    return pd.read_csv(source_file, index_col=0)

def save_profile(profile, target_file):
    if not os.path.isfile(target_file):
        df = pd.DataFrame([], columns=profile.keys())
        df.to_csv(target_file, index=True)
    df = pd.read_csv(target_file, index_col=0)
    df = df.append(profile, ignore_index=True)
    df.to_csv(target_file, index=True)


def merge_scraped_records(profile, company):
    merged = dict()
    merged.update(profile.__dict__)
    c_dict = company.__dict__
    for key in list(c_dict):
        if "company_" in key: continue
        c_dict["company_" + key] = c_dict.pop(key)
    merged.update(c_dict)
    return merged

def generalize_company_columns(company):
    company.headquarter = Generalizer.generalize_headquarter(company.headquarter)
    return company

if __name__ == "__main__":
    print("\n\n----------- Script started -----------\n")
    print(" Welcome to the emails scraper!\n")
    print(" The script will use links generated by the crawler\n")
    print(" It will visit LinkedIn profile and current company of every stored contact\n")
    print(" Please make sure you are logged in to LinkedIn in your Firefox\n")
    print(" Scraping parameters can be configured from 'network_mining_config.json'\n")
    print("\n--------------------------------------\n")
    print("Launching...\n")

    config = load_config()
    BATCH_SIZE = config["scraper_batch_size"]
    bookmark_file = os.path.join(DATA_FOLDER, config["scraper_bookmark_file_name"])
    target_data_file = os.path.join(DATA_FOLDER, config["scraper_result_file_name"])
    print("The output will be stored to: {}\n".format(target_data_file))

    browser = BrowserFactory.create()
    profileScraper = LinkedInProfileScraper(browser)
    companyScraper = LinkedInCompanyScraper(browser)
    bookmarkRepo = BookmarkRepo(bookmark_file)

    total = 0
    progress = 0
    return_code = os.EX_OK
    try:
        print("Visiting and scraping your next {} contacts\n".format(BATCH_SIZE))
        bookmark = bookmarkRepo.load_bookmark()
        print("Note: If your LinkedIn session has expired - open new Firefox window, login to LinkedIn, close Firefox window, restart the script.\n")

        profile_links = load_profile_links(config)

        total = len(profile_links)

        print("Bookmarked progress {} / {}\n".format(bookmark, total))

        for i in tqdm(range(bookmark, min(bookmark + BATCH_SIZE, total))):
            link_occupation = profile_links.iloc[i]["occupation"]
            link_name = profile_links.iloc[i]["name"]
            if ('8th light' in link_occupation.lower()):
                print("Skipping 8th-Lighter ", link_name)
                bookmarkRepo.save_bookmark(i)
                progress = i + 1
                continue

            url = profile_links.iloc[i]["link"]
            contact = profileScraper.scrape_contact_info(url)
            if('/8th-light' in contact.company_link):
                print("Skipping 8th-Lighter ", link_name)
                bookmarkRepo.save_bookmark(i)
                progress = i + 1
                continue

            company = EmptyScrapedCompany()
            if('/company/' in contact.company_link or
               '/school/' in contact.company_link):
               company_url = contact.company_link + "about"
               company = companyScraper.get_company_data(company_url)
               company = generalize_company_columns(company)

            profile = merge_scraped_records(contact, company)
            profile['probable_role'] = Generalizer.generalize_occupation(contact.occupation)
            
            save_profile(profile, target_data_file)
            bookmarkRepo.save_bookmark(i)
            progress = i + 1

    except Exception as ex:
        return_code = os.EX_DATAERR
        print("----------- ERROR -----------")
        print(ex)
        traceback.print_exc()
    finally:
        browser.quit()
        print("\nSaved progress:  {} / {}".format(progress, total))
        print("\n----------- Script finished -----------\n")
        sys.exit(return_code)
