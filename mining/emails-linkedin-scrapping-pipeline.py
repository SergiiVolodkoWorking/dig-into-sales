import os.path
import pandas as pd
from BrowserFactory import BrowserFactory
from BookmarkRepo import BookmarkRepo
from LinkedInProfileScrapper import LinkedInProfileScrapper
from LinkedInCompanyScrapper import LinkedInCompanyScrapper, EmptyScrappedCompany
import time
import json
import traceback
from tqdm import tqdm

root_folder = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
DATA_FOLDER = os.path.join(root_folder, "data")

def load_config():
    with open(os.path.join(os.path.dirname(__file__), 'network_mining_config.json')) as f:
        return json.load(f)

def load_profile_links(config):
    source_file = os.path.join(DATA_FOLDER, config["crawler_result_file_name"])
    if not os.path.isfile(source_file):
        raise Exception("crawler_result_file_name is not found by path:\n {}".format(source_file))
    return pd.read_csv(source_file, index_col=0)

def save_profile(profile, target_file):
    if not os.path.isfile(target_file):
        df = pd.DataFrame([], columns=profile.keys())
        df.to_csv(target_file, index=True)
    df = pd.read_csv(target_file, index_col=0)
    df = df.append(profile, ignore_index=True)
    df.to_csv(target_file, index=True)


def merge_scrapped_data(profile, company):
    merged = dict()
    merged.update(profile.__dict__)
    c_dict = company.__dict__
    for key in list(c_dict):
        if "company_" in key: continue
        c_dict["company_" + key] = c_dict.pop(key)
    
    merged.update(c_dict)
    return merged

if __name__ == "__main__":
    print("\n\n----------- Script started -----------\n")
    print(" Welcome to the emails scrapper!\n")
    print(" The script will use links generated by the crawler\n")
    print(" It will visit LinkedIn profile and current company of every stored contact\n")
    print(" Please make sure you are logged in to LinkedIn in your Firefox\n")
    print(" Scraping parameters can be configured from 'network_mining_config.json'\n")
    print("\n--------------------------------------\n")
    print("Launching...\n")

    config = load_config()
    BATCH_SIZE = config["scrapper_batch_size"]
    bookmark_file = os.path.join(DATA_FOLDER, config["scrapper_bookmark_file_name"])
    target_data_file = os.path.join(DATA_FOLDER, config["scrapper_result_file_name"])
    print("The output will be stored to: {}\n".format(target_data_file))

    browser = BrowserFactory.create()
    profileScrapper = LinkedInProfileScrapper(browser)
    companyScrapper = LinkedInCompanyScrapper(browser)
    bookmarkRepo = BookmarkRepo(bookmark_file)

    total = 0
    progress = 0
    try:
        print("Visiting and scrapping your next {} contacts".format(BATCH_SIZE))
        bookmark = bookmarkRepo.load_bookmark()

        profile_links = load_profile_links(config)

        total = len(profile_links)

        print("Bookmarked progress {} / {}\n".format(bookmark, total))
        
        for i in tqdm(range(bookmark, min(bookmark + BATCH_SIZE, total))):
            url = profile_links.iloc[i]["link"]
            contact = profileScrapper.scrap_contact_info(url)

            company = EmptyScrappedCompany()
            if('/company/' in contact.company_link or
               '/school/' in contact.company_link):
               company_url = contact.company_link + "about"
               company = companyScrapper.get_company_data(company_url)
            
            profile = merge_scrapped_data(contact, company)
            save_profile(profile, target_data_file)
            bookmarkRepo.save_bookmark(i)
            progress = i + 1

    except Exception as ex:
        print("----------- ERROR -----------")
        print(ex)
        traceback.print_exc()
    finally:
        browser.quit()
        print("\nProgress:  {} / {}".format(progress, total))
        print("\n----------- Script finished -----------\n")
